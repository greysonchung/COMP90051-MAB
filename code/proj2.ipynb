{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP90051 Project 2\n",
    "\n",
    "Haonan Zhong 867492\n",
    "\n",
    "**Copyright statement:** This notebook is copyright University of Melbourne. \n",
    "It is licensed for the sole purpose of your assessment in COMP90051. \n",
    "You are not permitted to share or publish derived versions of this notebook, other than with COMP90051 staff for assessment.\n",
    "\n",
    "***\n",
    "\n",
    "The code block below imports the namespaces/functions/classes you may use in the project. \n",
    "Additional imports are not permitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit. These are the only imports permitted.\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Type annotations\n",
    "from numpy import ndarray\n",
    "from numpy.random import Generator\n",
    "from typing import List, Optional, Tuple, Callable\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression  # for Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base `MAB` class below defines a common interface for a contextual multi-armed bandit. \n",
    "Your bandit implementations in Parts 1-4 should inherit from this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAB(ABC):\n",
    "    \"\"\"Base class for a contextual multi-armed bandit (MAB)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_arms : int\n",
    "        Number of arms.\n",
    "        \n",
    "    rng : Generator, optional\n",
    "        A `Generator` used as an internal source of randomness. If None, a \n",
    "        default `Generator` will be constructed using `np.random.default_rng`.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_arms: int, rng: Optional[Generator] = None) -> None:\n",
    "        if not n_arms >= 0:\n",
    "            raise ValueError(\"`n_arms` must be non-negative\")\n",
    "        self.n_arms = n_arms\n",
    "        self.rng = np.random.default_rng(rng)\n",
    "    \n",
    "    @abstractmethod\n",
    "    def play(self, context: Optional[ndarray] = None) -> int:\n",
    "        \"\"\"Play a round\n",
    "        \n",
    "        Parameters\n",
    "        ----------        \n",
    "        context : float numpy.ndarray, shape (n_arms, n_dims), optional\n",
    "            An array of context vectors presented to the MAB. The 0-th \n",
    "            axis indexes the arms, and the 1-st axis indexes the features.\n",
    "            Non-contextual bandits accept a context of None.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        arm : int\n",
    "            Integer index of the arm played this round. Should be in the set \n",
    "            {0, ..., n_arms - 1}.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def update(self, arm: int, reward: float, context: Optional[ndarray] = None) -> None:\n",
    "        \"\"\"Update the internal state of the MAB after a play\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        arm : int\n",
    "            Integer index of the played arm in the set {0, ..., n_arms - 1}.\n",
    "            \n",
    "        reward : float\n",
    "            Reward received from the arm.\n",
    "        \n",
    "        context : float numpy.ndarray, shape (n_arms, n_dims), optional\n",
    "            An array of context vectors that was presented to the MAB. The \n",
    "            0-th axis indexes the arms, and the 1-st axis indexes the \n",
    "            features. Non-contextual bandits accept a context of None.\n",
    "        \"\"\"\n",
    "        if not isinstance(arm, (int, np.integer)):\n",
    "            raise TypeError(\"`arm` must be an int\")\n",
    "        if arm >= self.n_arms or arm < 0:\n",
    "            raise ValueError(\"`arm` must be in the range [0, {}]\".format(self.n_arms - 1))\n",
    "        if not isinstance(reward, (int, np.integer, float, np.float64)):\n",
    "            raise TypeError(\"`reward` must be a numeric scalar\")\n",
    "            \n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def param_check(self, name, value, size=1, dtype=None, min_val=None, max_val=None):\n",
    "        \"\"\"Check whether the input value is correct, raising error if incorrect\n",
    "        \"\"\"\n",
    "        size = int(size)\n",
    "        if size < 0:\n",
    "            raise ValueError(\"\\\"size\\\" should be a positive integer\")\n",
    "        if len(value) != size:\n",
    "            raise ValueError(\"value size incorrect\")\n",
    "\n",
    "        if dtype == None or dtype == \"string\":\n",
    "            new_arg_value = np.array(value)\n",
    "        elif dtype == \"int\":\n",
    "            new_arg_value = np.array(value, dtype=\"int32\")\n",
    "        elif dtype == \"float\":\n",
    "            new_arg_value = np.array(value, dtype=\"float64\")\n",
    "        else:\n",
    "            raise ValueError(\"Unkown dytype: \" + str(dtype) + \", Argument: \" + str(name))\n",
    "\n",
    "        # check whether each element of arg is in the correct range\n",
    "        for i in range(size):\n",
    "            if min_value != None and new_arg_value[i] < float(min_val):\n",
    "                raise ValueError(\"Argument value less than \" + str(min_val) + \", Argument: \" + str(name))\n",
    "            \n",
    "            if max_value != None and new_arg_value[i] > float(max_val):\n",
    "                raise ValueError(\"Argument value greater than \" + str(max_val) + \", Argument: \" + str(name))\n",
    "        \n",
    "        return new_arg_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below implements off-policy evaluation as described in Appendix A of the project spec. \n",
    "You should use it—along with the provided dataset—to evaluate the bandits in Parts 1-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offline_eval(mab: MAB, arms: ndarray, rewards: ndarray, contexts: ndarray, \n",
    "                 n_rounds: Optional[int] = None) -> ndarray:\n",
    "    \"\"\"Offline evaluation of a multi-armed bandit\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mab : instance of MAB\n",
    "        MAB to evaluate.\n",
    "    \n",
    "    arms : int ndarray, shape (n_events,) \n",
    "        Array containing the history of pulled arms, represented as integer \n",
    "        indices in the set {0, ..., mab.n_arms}\n",
    "    \n",
    "    rewards : float ndarray, shape (n_events,)\n",
    "        Array containing the history of rewards. If a reward is missing, it \n",
    "        should be represented by `np.nan`.\n",
    "    \n",
    "    contexts : float ndarray, shape (n_events, n_arms, n_dims)\n",
    "        Array containing the history of contexts presented to the arms. \n",
    "        The 0-th axis indexes the events in the history, the 1-st axis \n",
    "        indexes the arms and the 2-nd axis indexed the features.\n",
    "        \n",
    "    n_rounds : int, optional\n",
    "        Number of matching events to evaluate the MAB on. If None, \n",
    "        continue evaluating until the historical events are exhausted.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    matching_rewards : ndarray\n",
    "        Rewards of matched events.\n",
    "    \"\"\"\n",
    "    # Check types\n",
    "    if not isinstance(mab, MAB):\n",
    "        raise TypeError(\"`mab` must be an instance of MAB\")\n",
    "    arms = np.asarray(arms)\n",
    "    rewards = np.asarray(rewards)\n",
    "    contexts = np.asarray(contexts)\n",
    "    if n_rounds is not None and n_rounds < 0:\n",
    "        raise ValueError(\"`n_rounds` must be non-negative\")\n",
    "    \n",
    "    # Check array dimensions\n",
    "    if arms.ndim != 1:\n",
    "        raise ValueError(\"`arms` must be a 1D array\")\n",
    "    if rewards.ndim != 1:\n",
    "        raise ValueError(\"`rewards` must be a 1D array\")\n",
    "    if contexts.ndim != 3:\n",
    "        raise ValueError(\"`contexts` must be a 3D array\")\n",
    "    if not (arms.shape[0] == rewards.shape[0] == contexts.shape[0]):\n",
    "        raise ValueError(\"first dimension of input arrays are inconsistent\")\n",
    "    if contexts.shape[1] != mab.n_arms:\n",
    "        raise ValueError(\"`contexts` has inconsistent second dimension\")\n",
    "    if arms.max() >= mab.n_arms or arms.min() < 0:\n",
    "        raise ValueError(\"`arms` contains ids that are out-of-range\")\n",
    "    \n",
    "    matched_ctr = 0\n",
    "    matched_ids = list()\n",
    "    for i in range(arms.size):\n",
    "        if n_rounds is not None and matched_ctr >= n_rounds: # Note: fixed\n",
    "            break\n",
    "        arm_id = mab.play(contexts[i])\n",
    "        if arm_id == arms[i]:\n",
    "            reward = None if np.isnan(rewards[i]) else rewards[i]\n",
    "            mab.update(arm_id, reward, contexts[i])\n",
    "            matched_ctr += 1\n",
    "            matched_ids.append(i)\n",
    "    \n",
    "    return rewards[matched_ids], matched_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please define any static functions/variables (used across multiple tasks) in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(90051)  # Random generator used throughout\n",
    "\n",
    "# Define additional static functions/variables here, if required\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Thompson sampling MAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TS(MAB):\n",
    "    \"\"\"Thompson sampling MAB with a Beta-Bernoulli reward model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_arms : int\n",
    "        Number of arms.\n",
    "\n",
    "    alpha0: float, optional\n",
    "        positive real prior hyperparameter\n",
    "\n",
    "    beta0: float, optional\n",
    "        positive real prior hyperparameter\n",
    "    \n",
    "    rng : Generator, optional\n",
    "        A `Generator` used as an internal source of randomness. If None, a \n",
    "        default `Generator` will be constructed using `np.random.default_rng`.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_arms: int, alpha0: float = 1.0, beta0: float = 1.0, \n",
    "                 rng: Optional[Generator] = None) -> None:\n",
    "        super().__init__(n_arms, rng)\n",
    "        self.alpha0 = float(alpha0)\n",
    "        self.beta0 = float(beta0)\n",
    "\n",
    "        if self.alpha0 <= 0 or self.beta0 <= 0:\n",
    "            raise ValueError(\"`beta` and `alpha` must be greater than 0\")\n",
    "\n",
    "        # intialise S and F, where Si is the posterior alpha update for arm i,\n",
    "        # and Fi is the posterior beta update for arm i\n",
    "        self.S = np.zeros(self.n_arms)\n",
    "        self.F = np.zeros(self.n_arms)\n",
    "        \n",
    "    def play(self, context: Optional[ndarray] = None) -> int:\n",
    "        super().play()\n",
    "        max_theta, max_arms = np.NINF, []\n",
    "\n",
    "        # iterate through all arms' estimation and find arms with the maximum value\n",
    "        for i in range(self.n_arms):\n",
    "            theta_i = np.random.beta(self.alpha0 + self.S[i], self.beta0 + self.F[i])\n",
    "\n",
    "            if theta_i == max_theta or abs(theta_i - max_theta) < 1e-6:\n",
    "                max_arms.append(i)\n",
    "            elif theta_i > max_theta:\n",
    "                max_theta = theta_i\n",
    "                max_arms = [i]\n",
    "\n",
    "        # break tie randomly, if there are multiple arms\n",
    "        return max_arms[np.random.randint(len(max_arms))]\n",
    "        \n",
    "    def update(self, arm: int, reward: float, context: Optional[ndarray] = None) -> None:\n",
    "        super().update(arm, reward)\n",
    "\n",
    "        if reward == 1:\n",
    "            self.S[arm] += 1\n",
    "        elif reward == 0:\n",
    "            self.F[arm] += 1\n",
    "        else:\n",
    "            raise ValueError(\"`reward` should be a positive integer has value either 0 or 1\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "file_path = \"../code/dataset.txt\"\n",
    "\n",
    "arms, rewards, features = [], [], []\n",
    "event_count = 0\n",
    "data = open(file_path, 'r')\n",
    "\n",
    "for line in data:\n",
    "    event_count += 1\n",
    "    cols = line.split()\n",
    "    arms.append(int(cols[0]))\n",
    "    rewards.append(float(cols[1]))\n",
    "\n",
    "    feature = cols[2:]\n",
    "    feature = [float(element) for element in feature]\n",
    "    features.append(feature)\n",
    "\n",
    "\n",
    "arms = np.array(arms)\n",
    "rewards = np.array(rewards)\n",
    "features = np.array(features)\n",
    "\n",
    "# reshape the context of each event into the required size\n",
    "contexts = features.reshape(event_count, 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TS average reward 0.22125\n"
     ]
    }
   ],
   "source": [
    "mab = TS(10, alpha0=1.0, beta0=1.0, rng=rng)\n",
    "TS_rewards, TS_ids = offline_eval(mab, arms, rewards, contexts, n_rounds=800)\n",
    "print('TS average reward', np.mean(TS_rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Thompson sampling contextual MAB with linear payoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinTS(MAB):\n",
    "    \"\"\"Thompson sampling contextual MAB with a ridge regression reward model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_arms : int\n",
    "        Number of arms.\n",
    "    \n",
    "    n_dims : int\n",
    "        Number of dimensions for each arm's context.\n",
    "        \n",
    "    v: float, optional\n",
    "        Positive real explore-exploit parameter\n",
    "    \n",
    "    rng : Generator, optional\n",
    "        A `Generator` used as an internal source of randomness. If None, a \n",
    "        default `Generator` will be constructed using `np.random.default_rng`.\n",
    "    \"\"\"    \n",
    "    def __init__(self, n_arms: int, n_dims: int, v: float = 1.0, \n",
    "                 rng: Optional[Generator] = None) -> None:\n",
    "        super().__init__(n_arms, rng)\n",
    "        self.n_dims = int(n_dims)\n",
    "        self.v = float(v)\n",
    "\n",
    "        if self.n_dims < 0:\n",
    "            raise ValueError(\"`n_dims` must be greater or equal 0\")\n",
    "        if self.v <= 0:\n",
    "            raise ValueError(\"`v` must be a positive real number\")\n",
    "\n",
    "        self.B = np.identity(n_dims)\n",
    "        self.mu_hat = np.array([np.zeros(self.n_dims)]).T\n",
    "        self.f = np.array([np.zeros(self.n_dims)]).T\n",
    "        \n",
    "    def play(self, context: Optional[ndarray] = None) -> int:\n",
    "        max_est, max_arms = np.NINF, []\n",
    "        mu_hat_1D = np.squeeze(np.asarray(self.mu_hat))\n",
    "        mu_sample = np.random.multivariate_normal(mu_hat_1D, np.dot(np.square(self.v), np.linalg.inv(self.B)))\n",
    "        mu_sample = np.matrix([mu_sample]).T\n",
    "        \n",
    "        for i in range(self.n_arms):\n",
    "            b_it = np.array(context[i]).T\n",
    "            est = np.dot(b_it.T, mu_sample)\n",
    "\n",
    "            if est == max_est or abs(est - max_est) < 1e-6:\n",
    "                max_arms.append(i)\n",
    "            elif est > max_est:\n",
    "                max_est = est\n",
    "                max_arms = [i]\n",
    "        \n",
    "        return max_arms[np.random.randint(len(max_arms))]\n",
    "\n",
    "    def update(self, arm: int, reward: float, context: Optional[ndarray] = None) -> None:\n",
    "        super().update(arm, reward)\n",
    "        b_at = np.array([context[arm]]).T\n",
    "\n",
    "        self.B += np.dot(b_at, b_at.T)\n",
    "        self.f += np.dot(reward, b_at)\n",
    "\n",
    "        # check whether B is invertible and update u_hat\n",
    "        try:\n",
    "            self.mu_hat = np.dot(np.linalg.inv(self.B), self.f)\n",
    "        except:\n",
    "            print(\"[ERROR]: matrix B not invertible, update failed, skip it\")\n",
    "            self.B -= np.dot(b_at, b_at.T)\n",
    "            self.f -= np.dot(reward, b_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinTS average reward 0.30375\n"
     ]
    }
   ],
   "source": [
    "mab = LinTS(10, 10, v=1.0, rng=rng)\n",
    "LinTS_rewards, LinTS_ids = offline_eval(mab, arms, rewards, contexts, n_rounds=800)\n",
    "print('LinTS average reward', np.mean(LinTS_rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Thompson sampling MABs with fair exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FairXTS(...):\n",
    "    \"\"\"FairX Thompson sampling MAB with a Beta-Bernoulli reward model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_arms : int\n",
    "        Number of arms.\n",
    "\n",
    "    c : float, optional\n",
    "        Scaling factor for expected reward in exponential merit function\n",
    "    \n",
    "    alpha0: float, optional\n",
    "        Positive real prior hyperparameter.\n",
    "\n",
    "    beta0: float, optional\n",
    "        Positive real prior hyperparameter.\n",
    "    \n",
    "    rng : Generator, optional\n",
    "        A `Generator` used as an internal source of randomness. If None, a \n",
    "        default `Generator` will be constructed using `np.random.default_rng`.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_arms: int, c: float = 1.0, alpha0: float = 1.0, \n",
    "                 beta0: float = 1.0, rng: Optional[Generator] = None) -> None:\n",
    "        ... # implement or remove this method\n",
    "    \n",
    "    def play(self, context: Optional[ndarray] = None) -> int:\n",
    "        ... # implement or remove this method\n",
    "    \n",
    "    def update(self, arm: int, reward: float, context: Optional[ndarray] = None) -> None:\n",
    "        ... # implement or remove this method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mab = FairXTS(10, c=1.0, alpha0=1.0, beta0=1.0, rng=rng)\n",
    "FairXTS_rewards, FairXTS_ids = offline_eval(mab, arms, rewards, contexts, n_rounds=800)\n",
    "print('FairX-TS average reward', np.mean(FairXTS_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use additional cells here for experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FairXLinTS(...):\n",
    "    \"\"\"FairX Thompson sampling contextual MAB with a ridge regression reward model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_arms : int\n",
    "        Number of arms.\n",
    "    \n",
    "    n_dims : int\n",
    "        Number of dimensions for each arm's context.\n",
    "    \n",
    "    c : float, optional\n",
    "        Scaling factor for expected reward in exponential merit function\n",
    "    \n",
    "    v: float, optional\n",
    "        Positive real explore-exploit parameter\n",
    "    \n",
    "    rng : Generator, optional\n",
    "        A `Generator` used as an internal source of randomness. If None, a \n",
    "        default `Generator` will be constructed using `np.random.default_rng`.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_arms: int, n_dims: int, c: float = 1.0, \n",
    "                 v: float = 1.0, rng: Optional[Generator] = None) -> None:\n",
    "        ... # implement or remove this method\n",
    "    \n",
    "    def play(self, context: Optional[ndarray] = None) -> int:\n",
    "        ... # implement or remove this method\n",
    "    \n",
    "    def update(self, arm: int, reward: float, context: Optional[ndarray] = None) -> None:\n",
    "        ... # implement or remove this method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mab = FairXLinTS(10, 10, c=1.0, v=1.0, rng=rng)\n",
    "FairXLinTS_rewards, FairXLinTS_ids = offline_eval(mab, arms, rewards, contexts, n_rounds=800)\n",
    "print('FairX-LinTS average reward', np.mean(FairXLinTS_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use additional cells here for experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: SquareCB contextual MAB with a logistic regression oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquareCB(...):\n",
    "    \"\"\"SquareCB contextual MAB with a logistic regression oracle\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_arms : int\n",
    "        Number of arms.\n",
    "\n",
    "    n_dims : int\n",
    "        Number of features for each arm's context.\n",
    "    \n",
    "    gamma : float, optional\n",
    "        Learning rate parameter.\n",
    "    \n",
    "    rng : Generator, optional\n",
    "        A `Generator` used as an internal source of randomness. If None, a \n",
    "        default `Generator` will be constructed using `np.random.default_rng`.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_arms: int, n_dims: int, gamma: float = 1.0, \n",
    "                 rng: Optional[Generator] = None) -> None:\n",
    "        ... # implement or remove this method\n",
    "    \n",
    "    def play(self, context: Optional[ndarray] = None) -> int:\n",
    "        ... # implement or remove this method\n",
    "    \n",
    "    def update(self, arm: int, reward: float, \n",
    "               context: Optional[ndarray] = None) -> None:\n",
    "        ... # implement or remove this method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mab = SquareCB(10, 10, gamma=18.0, rng=rng)\n",
    "SquareCB_rewards, SquareCB_ids = offline_eval(mab, arms, rewards, contexts, n_rounds=800)\n",
    "print('SquareCB average reward', np.mean(SquareCB_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use additional cells here for experimentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
